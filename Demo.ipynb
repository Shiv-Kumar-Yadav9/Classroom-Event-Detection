{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy_of_Demo.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tromXVGtd-bu",
        "colab_type": "text"
      },
      "source": [
        "**Initial settings to point the notebook to the location of the project datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhkPkrCfHDDD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/Final_Year')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlwDYK0nHZWY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('/content/Final_Year/My Drive/final')\n",
        "!ls -l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5aaZGdqetpq",
        "colab_type": "text"
      },
      "source": [
        "**Importing the basic libraries that will be needed in the code scripts.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iogbDRmNHZEU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.utils import np_utils\n",
        "from skimage.io import imread\n",
        "import pandas as pd\n",
        "from google.colab.patches import cv2_imshow\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2r2S_Ymje51i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "categories = ['sit', 'stand', 'walk', 'push']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYNzEyQIf0hr",
        "colab_type": "text"
      },
      "source": [
        "**Step1: Extract the frames from the video files and store it in directory.**\n",
        "\n",
        "*   One frame every third frame is extracted.\n",
        "*   10 such frames(F0 - F9) are extracted and stored ina directory named a0.\n",
        "*   Next 10 frames(F1 - F10) are stored in directory a1...and so on until frames are there in video.\n",
        "\n",
        "The frames are stored in a seperate directory called Demo with structure a{0-9}/img/img0 till img9.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79aGFszaGs8q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Save at /Demo/a*/img/img0-9.jpg    \n",
        "def extract_frames(directory):\n",
        "    video_paths = glob.glob(os.path.join('Dataset/Videos',directory, '*.avi'))\n",
        "    print('Extracting frames from %d %s videos' % (len(video_paths), 'Demo'))\n",
        "    image_id=0\n",
        "    folder_id=0\n",
        "    for video_path in video_paths:\n",
        "        video = cv2.VideoCapture(video_path)\n",
        "        frame_id = 0\n",
        "        while(video.isOpened()):\n",
        "            if(frame_id==0):\n",
        "                i=1\n",
        "                ret,frame10 = video.read()\n",
        "                while(i<3 and ret==True):\n",
        "                    ret,frame9 = video.read()\n",
        "                    i+=1\n",
        "                while(i>1 and ret==True):\n",
        "                    ret,frame8 = video.read()\n",
        "                    i-=1\n",
        "                while(i<3 and ret==True):\n",
        "                    ret,frame7 = video.read()\n",
        "                    i+=1\n",
        "                while(i>1 and ret==True):\n",
        "                    ret,frame6 = video.read()\n",
        "                    i-=1\n",
        "                while(i<3 and ret==True):\n",
        "                    ret,frame5 = video.read()\n",
        "                    i+=1\n",
        "                while(i>1 and ret==True):\n",
        "                    ret,frame4 = video.read()\n",
        "                    i-=1\n",
        "                while(i<3 and ret==True):\n",
        "                    ret,frame3 = video.read()\n",
        "                    i+=1\n",
        "                while(i>1 and ret==True):\n",
        "                    ret,frame2 = video.read()\n",
        "                    i-=1\n",
        "                while(i<3 and ret==True):\n",
        "                    ret,frame1 = video.read()\n",
        "                    i+=1\n",
        "            else:\n",
        "                frame10=frame9\n",
        "                frame9=frame8\n",
        "                frame8=frame7\n",
        "                frame7=frame6\n",
        "                frame6=frame5\n",
        "                frame5=frame4\n",
        "                frame4=frame3\n",
        "                frame3=frame2\n",
        "                frame2=frame1\n",
        "                i=1\n",
        "                while(i<3 and ret==True):\n",
        "                    ret,frame1 = video.read()\n",
        "                    i+=1\n",
        "            if(ret==False):\n",
        "                break\n",
        "            destination_directory = os.path.join('Dataset',directory, f'video{folder_id}','a'+str(image_id),'img')\n",
        "            os.makedirs(destination_directory, exist_ok=True)\n",
        "            image_path = os.path.join(destination_directory, 'img0' + '.jpg')\n",
        "            cv2.imwrite(image_path, frame1)\n",
        "            image_path = os.path.join(destination_directory, 'img1' + '.jpg')\n",
        "            cv2.imwrite(image_path, frame2)\n",
        "            image_path = os.path.join(destination_directory, 'img2' + '.jpg')\n",
        "            cv2.imwrite(image_path, frame3)\n",
        "            image_path = os.path.join(destination_directory, 'img3' + '.jpg')\n",
        "            cv2.imwrite(image_path, frame4)\n",
        "            image_path = os.path.join(destination_directory, 'img4' + '.jpg')\n",
        "            cv2.imwrite(image_path, frame5)\n",
        "            image_path = os.path.join(destination_directory, 'img5' + '.jpg')\n",
        "            cv2.imwrite(image_path, frame6)\n",
        "            image_path = os.path.join(destination_directory, 'img6' + '.jpg')\n",
        "            cv2.imwrite(image_path, frame7)\n",
        "            image_path = os.path.join(destination_directory, 'img7' + '.jpg')\n",
        "            cv2.imwrite(image_path, frame8)\n",
        "            image_path = os.path.join(destination_directory, 'img8' + '.jpg')\n",
        "            cv2.imwrite(image_path, frame9)\n",
        "            image_path = os.path.join(destination_directory, 'img9' + '.jpg')\n",
        "            cv2.imwrite(image_path, frame10)\n",
        "\n",
        "\n",
        "            image_id += 1\n",
        "            frame_id+=1\n",
        "\n",
        "        video.release()\n",
        "        print('Extracted %d frames from %s' % (image_id, video_path))\n",
        "        folder_id+=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P15Lx_2PG17q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "extract_frames('Demo')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJNrklc1kIXi",
        "colab_type": "text"
      },
      "source": [
        "**Step 2 : Detect individual person in each frame and get the coordinates of the person in the frame.**\n",
        "\n",
        "*  In this step the coordinates of each person is detected using the open source libraray called YOLO.\n",
        "\n",
        "* Person is searched for and detected in frame 0 ( F0 ) of each directory ,i.e. for each a{0-9}.\n",
        "\n",
        "*   The detected coordinates of the humans are then stored in a{0-9} folder in a .txt file named as person{0-9}.txt, where person0 stores the coordinates of first person in frame 0 ( F0 ) and person1 stores coordinates of second person in frame 0 ( F0 ) and so on...\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkDRiRqjbVvB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import cv2 as cv\n",
        "import argparse\n",
        "import sys\n",
        "import numpy as np\n",
        "import os.path\n",
        "\n",
        "# Initialize the parameters\n",
        "confThreshold = 0.3  # Confidence threshold\n",
        "nmsThreshold = 0.4  # Non-maximum suppression threshold\n",
        "inpWidth = 416  # Width of network's input image\n",
        "inpHeight = 416  # Height of network's input image\n",
        "\n",
        "# Load names of classes\n",
        "classesFile = \"coco.names\"\n",
        "classes = None\n",
        "with open(classesFile, 'rt') as f:\n",
        "    classes = f.read().rstrip('\\n').split('\\n')\n",
        "\n",
        "# Give the configuration and weight files for the model and load the network using them.\n",
        "modelConfiguration = \"yolov3.cfg\"\n",
        "modelWeights = \"yolov3.weights\"\n",
        "\n",
        "net = cv.dnn.readNetFromDarknet(modelConfiguration, modelWeights)\n",
        "net.setPreferableBackend(cv.dnn.DNN_BACKEND_OPENCV)\n",
        "net.setPreferableTarget(cv.dnn.DNN_TARGET_CPU)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MItKFOMfgEmF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the names of the output layers\n",
        "def getOutputsNames(net):\n",
        "    # Get the names of all the layers in the network\n",
        "    layersNames = net.getLayerNames()\n",
        "    # Get the names of the output layers, i.e. the layers with unconnected outputs\n",
        "    return [layersNames[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
        "\n",
        "\n",
        "# Remove the bounding boxes with low confidence using non-maxima suppression\n",
        "def postprocess(frame, outs, folder, li):\n",
        "    frameHeight = frame.shape[0]\n",
        "    frameWidth = frame.shape[1]\n",
        "    classIds = []\n",
        "    confidences = []\n",
        "    boxes = []\n",
        "    for out in outs:\n",
        "        for detection in out:\n",
        "            scores = detection[5:]\n",
        "            classId = np.argmax(scores)\n",
        "            confidence = scores[classId]\n",
        "            if confidence > confThreshold:\n",
        "                center_x = int(detection[0] * frameWidth)\n",
        "                center_y = int(detection[1] * frameHeight)\n",
        "                width = int(detection[2] * frameWidth)\n",
        "                height = int(detection[3] * frameHeight)\n",
        "                left = int(center_x - width / 2)\n",
        "                top = int(center_y - height / 2)\n",
        "                classIds.append(classId)\n",
        "                confidences.append(float(confidence))\n",
        "                boxes.append([left, top, width, height])\n",
        "\n",
        "    indices = cv.dnn.NMSBoxes(boxes, confidences, confThreshold, nmsThreshold)\n",
        "    box_values = []\n",
        "    p = 0;\n",
        "    for i in indices:\n",
        "        i = i[0]\n",
        "        box = boxes[i]\n",
        "        left = box[0]\n",
        "        top = box[1]\n",
        "        width = box[2]\n",
        "        height = box[3]\n",
        "\n",
        "        if classes[classIds[i]] == 'person':\n",
        "            dim = pd.DataFrame()\n",
        "            dim['l'] = [left]\n",
        "            dim['t'] = [top]\n",
        "            dim['w'] = width\n",
        "            dim['h'] = height\n",
        "            print(dim.head())\n",
        "            if (dim.empty):\n",
        "                print(folder)\n",
        "            else:\n",
        "                dim.to_csv(f'{folder}/person{p}.txt')\n",
        "            p += 1\n",
        "    if p == 0:\n",
        "        li.append(folder)\n",
        "        print(\"No person found in \", folder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIJLvNvUgMIb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def detect(frame, folder,li):\n",
        "    # Create a 4D blob from a frame.\n",
        "    blob = cv.dnn.blobFromImage(frame, 1 / 255, (inpWidth, inpHeight), [0, 0, 0], 1, crop=False)\n",
        "\n",
        "    # Sets the input to the network\n",
        "    net.setInput(blob)\n",
        "\n",
        "    # Runs the forward pass to get output of the output layers\n",
        "    outs = net.forward(getOutputsNames(net))\n",
        "\n",
        "    # Remove the bounding boxes with low confidence\n",
        "    postprocess(frame, outs,folder,li)\n",
        " \n",
        "    k = cv2.waitKey(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEIIBn7lgS_6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "from google.colab.patches import cv2_imshow\n",
        "categories=['sit','stand','walk','push']\n",
        "videos_data={'sit':9,'stand':9,'walk':12,'push':4}\n",
        "\n",
        "def detect_person(directory):\n",
        "    data = []\n",
        "    labels = []\n",
        "    per_category=[]\n",
        "    lists=[]\n",
        "    li=[]\n",
        "    videos=glob.glob(os.path.join('Dataset',directory,'video*'))\n",
        "    for video in videos:\n",
        "        folder_paths = glob.glob(os.path.join(video,'a*'))\n",
        "        for folder in folder_paths:\n",
        "            image_path = glob.glob(os.path.join(folder,'img', 'img0.jpg'))\n",
        "            frame = cv.imread(image_path[0])\n",
        "            detect(frame, folder,li)\n",
        "        lists.append(li)\n",
        "    return lists"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYNuJhUxhPCb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images_without_person_demo=detect_person('Demo')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObAN31HXm5B-",
        "colab_type": "text"
      },
      "source": [
        "There are few frames which do not contain any human in them and the folder names of such folders are stored in the variable *images_without_person_demo*  these folder can later be deleted manually or by using the shutil package of python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtCu3AI78Osy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images_without_person_demo"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjUMr1b3xbtt",
        "colab_type": "text"
      },
      "source": [
        "**Step 3: Track the coordinates of the person in the next 9 frames of the image set**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "*   MEEM tracker is used to detect the coordinates of the humans in the next 10 frames of the image set.\n",
        "*   It accepts the coordinates of the person in frame 0 ( F0 ), the initial frame ( F0 ) and the folder containing the set of 10 frames.\n",
        "\n",
        "\n",
        "*   It returns the tracked coordinates of that person in next 10 frames.\n",
        "\n",
        "*   Each output{0-9}.txt file containing the coordinates of the person in other 10 frames corresponds to every input{0-9}.txt files which contain the coordinates of person in first frame ( F0 ).\n",
        "\n",
        "\n",
        "NOTE: You need to place the MEEM tracker code in the same current directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09D0SvyazMs6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import matlab.engine\n",
        "import glob\n",
        "\n",
        "def track_from_matlab(directory):\n",
        "    eng=matlab.engine.start_matlab()\n",
        "    videos=glob.glob(os.path.join('D:/Dataset/',directory,'video*'))\n",
        "    for video in videos:\n",
        "        dirs=glob.glob(os.path.join(video,'a*'))\n",
        "        for dir in dirs:\n",
        "            text_files=glob.glob(os.path.join(dir, '*.txt'))\n",
        "            for i in range(len(text_files)):\n",
        "                jpg='jpg'\n",
        "                res=eng.MEEMTrack(str(i),dir,jpg,False,text_files[i])\n",
        "                print('From directory ',dir)\n",
        "                print('------------------------------------------------')\n",
        "    eng.quit()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtTbx4OP0zbv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "track_from_matlab('Demo')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpKFm8wTngiA",
        "colab_type": "text"
      },
      "source": [
        "**Step 4: Extract the person out of all 10 frames.**\n",
        "\n",
        "\n",
        "* The output file of MEEM tracker output{0-9}.txt contains the coordinates of the person{0-9} in all 10 frames that was detected in the YOLO stage.   \n",
        "\n",
        "*   The region bounded by the output{0-9}.txt files is cropped out from each 10 images and saved into a subfolder res{0-9}.\n",
        "\n",
        "*   Each individual person have their set of extracted images pertaining to that person only.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bgat9JXkh9kw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import time\n",
        "import cv2 as cv\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "def split_person(directory):\n",
        "    data = []\n",
        "    labels = []\n",
        "    per_category=[]\n",
        "    lists=[]\n",
        "    video_files=glob.glob(os.path.join('Dataset',directory,'video4'))\n",
        "    for video_file in video_files:\n",
        "        folder_paths = glob.glob(os.path.join(video_file,'a*'))\n",
        "        for folder in folder_paths:\n",
        "            txt_files=glob.glob(os.path.join(folder,'output*.txt'))\n",
        "            print(folder,len(txt_files))\n",
        "            for txt_file in txt_files:\n",
        "                crop_df=pd.read_csv(txt_file,names=['l','t','w','h'])\n",
        "                t=re.findall('\\d+',txt_file)[-1]\n",
        "                destination_directory = os.path.join(folder,f'res{t}')\n",
        "                os.makedirs(destination_directory, exist_ok=True)\n",
        "                for index, row in crop_df.iterrows():\n",
        "                    image_path = glob.glob(os.path.join(folder,'img', f'img{index}.jpg'))\n",
        "                    if image_path==[]:\n",
        "                        continue\n",
        "                    frame = cv.imread(image_path[0])\n",
        "                    t=int(row['t'])\n",
        "                    l=int(row['l'])\n",
        "                    h=int(row['h'])\n",
        "                    w=int(row['w'])\n",
        "                    ch=int(h/2)+t\n",
        "                    cw=int(w/2)+l\n",
        "                    if(h<300):\n",
        "                        h=300\n",
        "                    if(w<300):\n",
        "                        w=300\n",
        "                    t=ch-int(h/2)\n",
        "                    l=cw-int(w/2)\n",
        "                    crop=frame[t:t+h,l:l+w]\n",
        "                    crop_path = os.path.join(destination_directory, f'img{index}.jpg')\n",
        "                    cv2.imwrite(crop_path, crop)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BTa6NsvaxPO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "split_person('Demo')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNDqQH7NpRy5",
        "colab_type": "text"
      },
      "source": [
        "**Step 5: Get the input feature for Demo data.**\n",
        "\n",
        "\n",
        "*   Read the cropped images for each person. \n",
        "*   The images are then reshaped to 96\\*96 size and is converted into gray scale values.\n",
        "*   All the images are then concatenated together to give a structure of the shape 10 \\* 96 \\* 96 \\* 1.\n",
        "*   This formsas input to the CNN model which gives the probabiltiy of the event occuring.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JAP5hUsbHHw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_X_data(train_or_test):\n",
        "    data = []\n",
        "    per_category=[]\n",
        "    file_paths=pd.Series()\n",
        "    video_folders = glob.glob(os.path.join('./Dataset/Demo', 'video3'))    \n",
        "    for video_folder in video_folders:\n",
        "        folders = glob.glob(os.path.join(video_folder, 'a*'))\n",
        "        print(\"%3s. %-7d files\" % (video_folder, len(folders)))\n",
        "        per_category.append(len(folders))\n",
        "        for folder in folders:\n",
        "            results = glob.glob(os.path.join(folder, 'res*'))\n",
        "            for res in results:\n",
        "                try:\n",
        "                    fdi=[]\n",
        "                    image1 = cv2.imread(os.path.join(res,'img0.jpg'))\n",
        "                    image2 = cv2.imread(os.path.join(res,'img1.jpg'))\n",
        "                    image3 = cv2.imread(os.path.join(res,'img2.jpg'))\n",
        "                    image4 = cv2.imread(os.path.join(res,'img3.jpg'))\n",
        "                    image5 = cv2.imread(os.path.join(res,'img4.jpg'))\n",
        "                    image6 = cv2.imread(os.path.join(res,'img5.jpg'))\n",
        "                    image7 = cv2.imread(os.path.join(res,'img6.jpg'))\n",
        "                    image8 = cv2.imread(os.path.join(res,'img7.jpg'))\n",
        "                    image9 = cv2.imread(os.path.join(res,'img8.jpg'))\n",
        "                    image10 = cv2.imread(os.path.join(res,'img9.jpg'))\n",
        "                    image1 = cv2.resize(image1, (96,96))\n",
        "                    image2 = cv2.resize(image2, (96,96))\n",
        "                    image3 = cv2.resize(image3, (96,96))\n",
        "                    image4 = cv2.resize(image4, (96,96))\n",
        "                    image5 = cv2.resize(image5, (96,96))\n",
        "                    image6 = cv2.resize(image6, (96,96))\n",
        "                    image7 = cv2.resize(image7, (96,96))\n",
        "                    image8 = cv2.resize(image8, (96,96))\n",
        "                    image9 = cv2.resize(image9, (96,96))\n",
        "                    image10 = cv2.resize(image10, (96,96))\n",
        "                    image1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
        "                    image2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
        "                    image3 = cv2.cvtColor(image3, cv2.COLOR_BGR2GRAY)\n",
        "                    image4 = cv2.cvtColor(image4, cv2.COLOR_BGR2GRAY)\n",
        "                    image5 = cv2.cvtColor(image5, cv2.COLOR_BGR2GRAY)\n",
        "                    image6 = cv2.cvtColor(image6, cv2.COLOR_BGR2GRAY)\n",
        "                    image7 = cv2.cvtColor(image7, cv2.COLOR_BGR2GRAY)\n",
        "                    image8 = cv2.cvtColor(image8, cv2.COLOR_BGR2GRAY)\n",
        "                    image9 = cv2.cvtColor(image9, cv2.COLOR_BGR2GRAY)\n",
        "                    image10 = cv2.cvtColor(image10, cv2.COLOR_BGR2GRAY)\n",
        "                    image=np.concatenate((image1,image2,image3,image4,image5,image6,image7,image8,image9,image10)).reshape(-1,96,96,1)\n",
        "                    data.append(image)\n",
        "                    file_paths=file_paths.append(pd.Series(res))\n",
        "                except Exception as e:\n",
        "                    print('Error in %s'% res)\n",
        "                    print(e)\n",
        "    X = np.array(data)\n",
        "    X = X / 255.\n",
        "    file_paths=file_paths.reset_index(drop=True)\n",
        "    \n",
        "    print('X_%s.shape:' % train_or_test, X.shape)\n",
        "    print('per_%s_file.shape:' % train_or_test, per_category)\n",
        "    print('total files %d' % len(file_paths))\n",
        "    return X,file_paths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnVrlFc0bUX7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_demo, file_paths = load_X_data('Demo')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrVgAvgtqxW4",
        "colab_type": "text"
      },
      "source": [
        "The input features can be saved into a pickle file inorder to avoid recompution of the features in case of timeout on Google Colab or some other local computation reasons."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlRghC15IOiD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import pickle\n",
        "# with open('demo96.pickle', 'wb') as f:\n",
        "#     pickle.dump([X_demo,file_paths], f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QR9WkObPIhjs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "with open('demo96.pickle', 'rb') as f:\n",
        "    X_demo,file_paths = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbCB5pZLbcXP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv3D, MaxPooling3D, GlobalAveragePooling3D\n",
        "from keras.layers.core import Dense, Activation, Dropout\n",
        "\n",
        "def load_model(inputShape,classes):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv3D(filters=32, kernel_size=(1, 4, 4), strides=(2, 1, 1), padding='valid', activation='relu', \n",
        "                     input_shape=inputShape))\n",
        "    model.add(MaxPooling3D(pool_size=2, strides=(1, 2, 2), padding='same'))\n",
        "\n",
        "    model.add(Conv3D(filters=64, kernel_size=(1, 4, 4), strides=(2, 1, 1), padding='valid', activation='relu'))\n",
        "    model.add(MaxPooling3D(pool_size=2, strides=(1, 2, 2), padding='same'))\n",
        "\n",
        "    model.add(Dropout(0.4))    \n",
        "    \n",
        "    model.add(Conv3D(filters=128, kernel_size=(1, 4, 4), strides=(2, 1, 1), padding='valid', activation='relu'))\n",
        "    model.add(MaxPooling3D(pool_size=2, strides=(1, 2, 2), padding='same'))\n",
        "\n",
        "    model.add(Conv3D(filters=64, kernel_size=(1, 2, 2), strides=(2, 1, 1), padding='valid', activation='relu'))\n",
        "    model.add(MaxPooling3D(pool_size=2, strides=(1, 2, 2), padding='same'))\n",
        "    \n",
        "    model.add(Conv3D(filters=32, kernel_size=(1, 2, 2), strides=(2, 1, 1), padding='valid', activation='relu'))\n",
        "    model.add(MaxPooling3D(pool_size=2, strides=(1, 2, 2), padding='same'))\n",
        "    \n",
        "    model.add(Dropout(0.3))    \n",
        "    \n",
        "    model.add(GlobalAveragePooling3D())\n",
        "\n",
        "    model.add(Dropout(0.3))\n",
        "    \n",
        "    model.add(Dense(8, activation='relu'))\n",
        "\n",
        "    model.add(Dense(classes, activation='softmax'))\n",
        "\n",
        "    print(model.summary())\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwZGIc1_sK-f",
        "colab_type": "text"
      },
      "source": [
        "**Two strategies were deployed and checked for better results**\n",
        "\n",
        "\n",
        "*   One 3DCNN model that gives prediction for all the classes of events (sit, stand, walk, push) that could possibly occur.\n",
        "*   Four seperate 3DCNN models that individually checked for probabilty of occurance of each of the individual class of event.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zs8L8Wlqs3wr",
        "colab_type": "text"
      },
      "source": [
        "**Model 1: One model for all class of events.**\n",
        "\n",
        "\n",
        "*   The model outptus the probability of the set of images to represent each of the classes of event.\n",
        "*   The event which has highest probability is selected as the event that is occuring and is selected for labelling on the image.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbIldlMybj14",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model=load_model((10, 96, 96, 1),4)\n",
        "\n",
        "# Compiling the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.load_weights('Dataset/result1/Model_1_10_96_96_4.weights.best.hdf5')\n",
        "\n",
        "predictions = model1.predict_proba(X_demo)\n",
        "predictions=np.argmax(predictions,axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx84pciBtmPZ",
        "colab_type": "text"
      },
      "source": [
        "**Model 2: Four seperate model for each class of events.**\n",
        "\n",
        "\n",
        "*   Each of the four model outptus the probability of the set of images to represent that classes of event.\n",
        "*   Out of all thet probabilities the probability which is highest is selected as the event that is occuring and is selected for labelling on the image.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VVz48JMFjyS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model1=load_model((10, 96, 96, 1),2)\n",
        "model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model1.load_weights('Dataset/result2/Model_1_10_96_96_2.weights.best.hdf5')\n",
        "\n",
        "predictions1 = model1.predict_proba(X_demo)[:,1]\n",
        "\n",
        "model2=load_model((10, 96, 96, 1),2)\n",
        "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model2.load_weights('Dataset/result2/Model_2_10_96_96_2.weights.best.hdf5')\n",
        "\n",
        "predictions2 = model2.predict_proba(X_demo)[:,1]\n",
        "\n",
        "model3=load_model((10, 96, 96, 1),2)\n",
        "model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model3.load_weights('Dataset/result2/Model_3_10_96_96_2.weights.best.hdf5')\n",
        "\n",
        "predictions3 = model3.predict_proba(X_demo)[:,1]\n",
        "\n",
        "model4=load_model((10, 96, 96, 1),2)\n",
        "model4.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model4.load_weights('Dataset/result2/Model_4_10_96_96_2.weights.best.hdf5')\n",
        "\n",
        "predictions4 = model4.predict_proba(X_demo)[:,1]\n",
        "\n",
        "print(len(predictions1),len(predictions2),len(predictions3),len(predictions4))\n",
        "predictions = [predictions1] + [predictions2] + [predictions3] + [predictions4]\n",
        "print(len(predictions))\n",
        "predictions = list(map(list, zip(*predictions)))\n",
        "print(len(predictions))\n",
        "predictions=np.argmax(predictions,axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOMF0SiwuGbB",
        "colab_type": "text"
      },
      "source": [
        "**Step 6: Labelling the images and generating video.**\n",
        "\n",
        "\n",
        "1.   In this step the frame 0 ( F0 ) of all the folders is labelled with corresponding predition and the image labelled for each human detected is stored in the directory names 'res'.\n",
        "2.   The images are stored in the alphabetical order of how they occur in the video and thus have the same order in which they can be joined to form a video.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSNFN1Vdgmte",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import cv2 as cv\n",
        "categories=['sit', 'stand', 'walk', 'push']\n",
        "def draw_boxes(file_paths, predictions):\n",
        "    file_paths.sort_values(inplace=True)\n",
        "    video_folders = glob.glob(os.path.join('./Dataset/Demo', 'video*'))    \n",
        "    for video_folder in video_folders:\n",
        "        video_data=file_paths[file_paths.str.contains(video_folder+'/')]\n",
        "        destination_directory = os.path.join(video_folder,'res')\n",
        "        os.makedirs(destination_directory, exist_ok=True)\n",
        "                \n",
        "        frame_folders = glob.glob(os.path.join(video_folder,'a*'))    \n",
        "        for i in range(len(frame_folders)):\n",
        "            frame_data=video_data[video_data.str.contains(frame_folders[i]+'/')]\n",
        "            if frame_data.empty:\n",
        "                continue\n",
        "            print(frame_data)\n",
        "            print(re.findall('\\d+',frame_data.iloc[0])[1])\n",
        "            person_no=frame_data.apply(lambda x: re.search('\\d+$',x).group(0))\n",
        "            person_files=person_no.apply(lambda x : '/output' + x + '.txt')\n",
        "            \n",
        "            image_path = os.path.join(frame_folders[i],'img','img0.jpg')\n",
        "            frame=cv2.imread(image_path)\n",
        "            img_no=re.findall('\\d+',frame_data.iloc[0])[1]\n",
        "            save_path = os.path.join(destination_directory, 'img'+img_no+'.jpg')\n",
        "            print('---------------------------------------')\n",
        "            for index,row in person_files.items():\n",
        "                label=categories[predictions[index]]\n",
        "                df=pd.read_csv(frame_folders[i]+row)\n",
        "                left=int(df.iloc[0][0])\n",
        "                top=int(df.iloc[0][1])\n",
        "                width=int(df.iloc[0][2])\n",
        "                height=int(df.iloc[0][3])\n",
        "                right= left+width\n",
        "                bottom=top+height               \n",
        "                cv.rectangle(frame, (left, top), (right, bottom), (255, 178, 50), 3)\n",
        "                labelSize, baseLine = cv.getTextSize(label, cv.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n",
        "                top = max(top, labelSize[1])\n",
        "                cv.rectangle(frame, (left, top - round(1.5 * labelSize[1])), (left + round(1.5 * labelSize[0]), top + baseLine),\n",
        "                             (255, 255, 255), cv.FILLED)\n",
        "                cv.putText(frame, label, (left, top), cv.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 0), 1)\n",
        "                \n",
        "            cv.imwrite(save_path, frame)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZtcYki8nl2J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "draw_boxes(file_paths,predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}